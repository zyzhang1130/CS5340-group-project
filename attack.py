# -*- coding: utf-8 -*-
"""attack.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jr7IbRTmPeJD40sliDG923iyJKAb5yMa
"""

import time

import eagerpy as ep
import foolbox as fb
import foolbox.attacks as fa
import matplotlib.pyplot as plt
import torch
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST


def main():
    # Load your trained model as fb.PyTorchModel here
    integrated = Integrated()
    integrated = integrated.eval()
    fmodel = fb.PyTorchModel(integrated, bounds=(0, 1))

    test_size = 1000  # assuming we are running 1000 tests for each attack
    batch_size = 50   # adjust your batch size according to memory limitation, as long as it's a divisor of 1000
    num_batch = int(test_size / batch_size)

    test_dataset = MNIST(root='./data', download=True, train=False, transform=transforms.ToTensor())
    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    Linf_epsilons = [
        0.1,
        0.2,
        0.3,
        0.4,
        0.5
    ]
    CW_epsilons = [
        0.1,
        1,
        10,
        100,
        1000
    ]
    L2_epsilons = [
        1,
        2,
        3,
        4,
        5
    ]
    attacks = [
        # gradient-based attacks
        {"name": "Fast Gradient Method", "model": fa.L2FastGradientAttack(), "epsilons": L2_epsilons},
        {"name": "Fast Gradient Sign Method", "model": fa.LinfFastGradientAttack(), "epsilons": Linf_epsilons},
        {"name": "L2 Projected Gradient Descent", "model": fa.L2ProjectedGradientDescentAttack(), "epsilons": L2_epsilons},
        {"name": "Linf Projected Gradient Descent", "model": fa.LinfProjectedGradientDescentAttack(), "epsilons": Linf_epsilons},
        {"name": "Carlini & Wagner L2 Attack", "model": fa.L2CarliniWagnerAttack(), "epsilons": CW_epsilons},
        # # decision-based attacks
        # {"name": "Boundary Attack", "model": fa.BoundaryAttack(), "epsilons": L2_epsilons}, 
        {"name": "Additive Uniform Noise Attack", "model": fa.LinfAdditiveUniformNoiseAttack(), "epsilons": Linf_epsilons},
        {"name": "Additive Gaussian Noise Attack", "model": fa.L2AdditiveGaussianNoiseAttack(), "epsilons": L2_epsilons},
        {"name": "Linear Search Contrast Reduction Attack", "model": fa.LinearSearchContrastReductionAttack(distance=distances.linf), "epsilons": Linf_epsilons}, 
        {"name": "Binary Search Contrast Reduction Attack", "model": fa.BinarySearchContrastReductionAttack(distance=distances.linf), "epsilons": Linf_epsilons}, 
        {"name": "Gaussian Blur Attack", "model": fa.GaussianBlurAttack(distance=distances.linf), "epsilons": Linf_epsilons}
    ]

    fig, axs = plt.subplots(len(attacks))
    fig.tight_layout()

    for i, attack in enumerate(attacks):
        clean_accuracy = torch.zeros(num_batch)
        robust_accuracy_batch = torch.zeros([num_batch, len(L2_epsilons)])
        count = 0
        t1 = time.time()
        for images, labels in test_dataloader:
            if count == num_batch:
                break
            images = ep.astensor(images)
            labels = ep.astensor(labels)
            clean_accuracy[count] = fb.utils.accuracy(fmodel, images, labels)

            # The raw adversarial examples. This depends on the attack and we cannot make an guarantees about this output.
            # The clipped adversarial examples. These are guaranteed to not be perturbed more than epsilon and thus are the actual adversarial examples you want to visualize. Note that some of them might not actually switch the class. To know which samples are actually adversarial, you should look at the third tensor.
            # The third tensor contains a boolean for each sample, indicating which samples are true adversarials that are both misclassified and within the epsilon balls around the clean samples.
            raw_adv, clipped_adv, is_adv = attack["model"](fmodel, images, labels, epsilons=attack["epsilons"])

            success_ = is_adv.numpy()
            robust_accuracy_batch[count] = torch.from_numpy(1.0 - success_.mean(axis=-1))

            count += 1

        t2 = time.time()
        print(attack)
        print("Clean accuracy: {:.5f}".format(torch.mean(clean_accuracy)))
        print("Average time taken for each test: {} seconds".format((t2 - t1) / test_size))
        robust_accuracy = torch.mean(robust_accuracy_batch, dim=0)
        print("Accuracy with {} attack: {}".format(attack["name"], robust_accuracy))

        # plot the robust accuracy against epsilons for the particular attack
        axs[i].set_ylim([0, 1])
        axs[i].plot(attack["epsilons"], robust_accuracy.numpy())
        axs[i].set_title(attack["name"])


if __name__ == '__main__':
    main()